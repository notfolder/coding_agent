# Ollama API 仕様

本ドキュメントは、Ollama APIの概要と使用方法を日本語でまとめたものです。

---

## 1. 概要

### 1.1 Ollamaとは

Ollamaは、ローカル環境で大規模言語モデル（LLM）を簡単に実行するためのツールです。様々なオープンソースモデルをダウンロードして実行でき、REST APIを通じてアクセスできます。

### 1.2 主な機能

- **ローカルLLM実行**: コマンドラインから簡単にモデルを実行
- **REST API**: HTTP経由でのモデルアクセス
- **モデル管理**: モデルのpull、push、リスト表示、削除
- **カスタムモデル**: Modelfileを使用したカスタムモデルの作成

---

## 2. 前提条件

### 2.1 インストール

Ollamaをインストールして実行している必要があります。公式サイト（https://ollama.com/download）からダウンロードできます。

### 2.2 モデルのダウンロード

使用するモデルを事前にダウンロードする必要があります。ollama pullコマンドを使用してモデルをダウンロードします。利用可能なモデルはOllama公式サイト（https://ollama.com/search）で確認できます。

---

## 3. チャットAPI

### 3.1 基本的な使い方

Ollamaのchat関数を使用して、チャット形式でモデルと対話します。

### 3.2 リクエストの構成

チャットリクエストには以下のパラメータを指定します：

- **model**: 使用するモデル名（必須）
- **messages**: メッセージのリスト（必須）
  - 各メッセージにはroleとcontentを含める

### 3.3 レスポンスの取得

レスポンスオブジェクトから応答内容を取得できます。messageフィールドに応答メッセージが含まれます。

---

## 4. ストリーミング応答

### 4.1 概要

streamパラメータをtrueに設定することで、レスポンスのストリーミングを有効にできます。

### 4.2 使用方法

ストリーミングを有効にすると、応答がチャンク単位で返されます。各チャンクを処理することで、リアルタイムに応答を表示できます。

---

## 5. カスタムクライアント

### 5.1 クライアントの作成

ClientまたはAsyncClientをインスタンス化してカスタムクライアントを作成できます。

### 5.2 設定オプション

- **host**: Ollamaサーバーのホスト（デフォルト：http://localhost:11434）
- **headers**: カスタムHTTPヘッダー

---

## 6. 非同期クライアント

### 6.1 概要

AsyncClientクラスを使用して非同期リクエストを行うことができます。Clientクラスと同じフィールドで設定できます。

### 6.2 ストリーミング

非同期クライアントでストリーミングを有効にすると、Python非同期ジェネレータとして応答が返されます。

---

## 7. API機能一覧

### 7.1 チャット・生成

- **chat**: チャット形式でモデルと対話
- **generate**: プロンプトからテキストを生成

### 7.2 モデル管理

- **list**: 利用可能なモデルを一覧表示
- **show**: モデルの詳細情報を表示
- **pull**: モデルをダウンロード
- **push**: モデルをアップロード
- **copy**: モデルをコピー
- **delete**: モデルを削除
- **create**: カスタムモデルを作成

### 7.3 埋め込み

- **embed**: テキストの埋め込みベクトルを生成
- 単一テキストまたはバッチ処理に対応

### 7.4 システム

- **ps**: 実行中のモデルを表示

---

## 8. エラーハンドリング

### 8.1 エラーの種類

リクエストがエラーステータスを返した場合、またはストリーミング中にエラーが検出された場合にエラーが発生します。

### 8.2 エラー処理

ResponseErrorをキャッチしてエラーを処理します。status_codeでエラーの種類を判別できます（例：404はモデルが存在しない）。

---

## 9. 本プロジェクトでの使用

### 9.1 使用方法

本プロジェクトのOllamaClientクラスでは、OllamaのREST APIを使用してLLMとの対話を行います。

### 9.2 設定項目

config.yamlのllm.ollamaセクションで以下を設定します：

- **endpoint**: OllamaサーバーのURL（デフォルト：http://localhost:11434）
- **model**: 使用するモデル
- **max_token**: 最大トークン数

### 9.3 起動方法

1. Ollamaをインストール
2. 使用するモデルをpull（ollama pull llama3.2など）
3. Ollamaサービスを起動（ollama serve）
4. config.yamlでllm.providerをollamaに設定

---

**参照元**: Ollama公式ドキュメント（https://github.com/ollama/ollama/blob/main/docs/api.md）
