# LM Studio API 仕様

本ドキュメントは、LM Studio APIの概要と使用方法を日本語でまとめたものです。

---

## 1. 概要

### 1.1 LM Studioとは

LM Studioは、ローカル環境で大規模言語モデル（LLM）を実行するためのソフトウェアです。OpenAI互換のAPIサーバーを提供し、様々なオープンソースモデルをローカルで実行できます。

### 1.2 主な機能

- **ローカルLLM実行**: GPUを使用してローカルでモデルを実行
- **OpenAI互換API**: OpenAI APIと互換性のあるエンドポイントを提供
- **モデル管理**: 様々なオープンソースモデルのダウンロードと管理
- **ストリーミング応答**: リアルタイムでの応答取得

---

## 2. チャット補完

### 2.1 基本的な使い方

LM Studioのllm.respondメソッドを使用して、チャット会話に対する補完を生成します。

### 2.2 応答の取得

モデルハンドルを取得し、respondメソッドにプロンプトを渡すことで、AIからの応答を得ることができます。

### 2.3 ストリーミング応答

respond_streamメソッドを使用すると、応答全体を待たずにテキストフラグメントを受信しながら表示できます。

---

## 3. モデルの取得

### 3.1 モデルハンドルの取得

トップレベルのllm便利APIまたはスコープリソースAPIのllmネームスペースのmodelメソッドを使用してモデルハンドルを取得します。

### 3.2 モデルの指定

特定のモデル（例：qwen2.5-7b-instruct）を使用する場合は、モデル名を引数として指定します。

---

## 4. チャットコンテキスト管理

### 4.1 コンテキストの概念

モデルへの入力は「コンテキスト」と呼ばれます。概念的には、モデルはマルチターンの会話を入力として受け取り、その会話でのアシスタントの応答を予測するよう求められます。

### 4.2 チャットの作成

初期システムプロンプトを指定してChatオブジェクトを作成し、ユーザーメッセージやアシスタントメッセージを追加してコンテキストを構築します。

---

## 5. 応答の生成

### 5.1 非ストリーミング応答

respondメソッドを使用して、チャットコンテキストに対する次の応答を予測させます。

### 5.2 ストリーミング応答

respond_streamメソッドを使用すると、応答をリアルタイムで受信できます。

---

## 6. 推論パラメータのカスタマイズ

### 6.1 設定可能なパラメータ

respondメソッドのconfigキーワードパラメータを通じて推論パラメータを渡すことができます。

### 6.2 主要なパラメータ

- **temperature**: 出力のランダム性を制御（0.0〜1.0）
- **maxTokens**: 生成する最大トークン数

---

## 7. 予測統計

### 7.1 取得可能な統計情報

予測結果から以下のメタデータを取得できます：

- **model_info.display_name**: 生成に使用されたモデル
- **stats.predicted_tokens_count**: 生成されたトークン数
- **stats.time_to_first_token_sec**: 最初のトークンまでの時間（秒）
- **stats.stop_reason**: 停止理由

---

## 8. マルチターンチャット

### 8.1 概要

対話的なチャットボットを実装するには、会話履歴を管理しながらユーザー入力を処理し、モデルの応答を取得します。

### 8.2 実装の流れ

1. モデルハンドルを取得
2. システムプロンプトを設定したChatオブジェクトを作成
3. ユーザー入力を受け取りチャットに追加
4. respond_streamで応答を取得
5. 応答をチャットに追加
6. 繰り返し

---

## 9. 本プロジェクトでの使用

### 9.1 使用方法

本プロジェクトのLMStudioClientクラスでは、LM StudioのOpenAI互換APIを使用してLLMとの対話を行います。

### 9.2 設定項目

config.yamlのllm.lmstudioセクションで以下を設定します：

- **base_url**: LM StudioサーバーのURL（デフォルト：localhost:1234）
- **model**: 使用するモデル
- **context_length**: コンテキスト長

### 9.3 起動方法

1. LM Studioアプリケーションを起動
2. 使用するモデルをロード
3. サーバーを開始（デフォルトポート：1234）
4. config.yamlでllm.providerをlmstudioに設定

---

**参照元**: LM Studio公式ドキュメント（https://lmstudio.ai/docs）
