mcp_servers:
  - mcp_server_name: "github"
    command:
      - "/app/github-mcp-server.cmd"
      - "stdio"
    env:
      GITHUB_TOOLSETS: "all"

  - mcp_server_name: "gitlab"
    command:
      - "npx"
      - "@zereight/mcp-gitlab"
      - "stdio"

llm:
  provider: "openai"    # "ollama" | "openai"
  function_calling: true
  lmstudio:
    base_url: "host.docker.internal:1234"
    # base_url: "localhost:1234"
    context_length: 32768
    model: "qwen3-30b-a3b-mlx"
  ollama:
    endpoint: "http://host.docker.internal:11434"
    model: "qwen3-30b-a3b-mlx"
    max_token: 32768
  openai:
    base_url: "https://api.openai.com/v1"
    api_key: "OPENAI_API_KEY"
    model: "gpt-4o"
    max_token: 40960
max_llm_process_num: 1000

github:
  owner:     "notfolder"
  bot_label: "coding agent"
  processing_label: "coding agent processing"
  done_label: "coding agent done"
  query: 'state:open archived:false sort:updated-desc sort:updated-desc'

gitlab:
  owner:     "notfolder"
  bot_label: "coding agent"
  processing_label: "coding agent processing"
  done_label: "coding agent done"
  project_id: "coding-agent-project"
  query: ''

scheduling:
  interval: 300  # 秒

# RabbitMQを使う場合はtrue、使わない場合はfalse
use_rabbitmq: true
rabbitmq:
  host: host.docker.internal
  port: 5672
  user: guest
  password: guest
  queue: coding_agent_tasks
